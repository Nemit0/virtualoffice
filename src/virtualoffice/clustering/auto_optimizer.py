"""
Auto-optimizer for clustering parameters using GPT-based quality evaluation.

This module tries different parameter combinations and uses GPT to evaluate
which clustering produces the most coherent, meaningful clusters.
"""

import logging
from dataclasses import dataclass
from datetime import datetime
from typing import Optional

from virtualoffice.clustering.cluster_engine import ClusterEngine
from virtualoffice.clustering import db
from virtualoffice.clustering.models import ClusterMetadata
from virtualoffice.utils.completion_util import generate_text

logger = logging.getLogger(__name__)


@dataclass
class ParameterConfig:
    """A configuration of clustering parameters to test."""
    dbscan_eps: float
    dbscan_min_samples: int
    tsne_perplexity: float

    def __str__(self):
        return f"eps={self.dbscan_eps}, min_samples={self.dbscan_min_samples}, perplexity={self.tsne_perplexity}"


@dataclass
class ClusterQuality:
    """Quality metrics for a clustering result."""
    config: ParameterConfig
    total_emails: int
    num_clusters: int
    noise_percentage: float
    coherence_score: float  # 0-10 from GPT evaluation
    distribution_score: float  # 0-10 based on cluster size distribution
    overall_score: float  # Weighted combination
    evaluation_details: str  # GPT's reasoning

    def __str__(self):
        return (
            f"Config: {self.config}\n"
            f"  Clusters: {self.num_clusters}, Noise: {self.noise_percentage:.1f}%\n"
            f"  Coherence: {self.coherence_score:.1f}/10, Distribution: {self.distribution_score:.1f}/10\n"
            f"  Overall Score: {self.overall_score:.1f}/10"
        )


def generate_parameter_grid(num_emails: int) -> list[ParameterConfig]:
    """
    Generate a grid of parameter configurations to test.

    The grid is tailored based on dataset size:
    - Small datasets (< 100 emails): Tighter clustering
    - Medium datasets (100-300 emails): Balanced
    - Large datasets (> 300 emails): Can use looser clustering
    """
    if num_emails < 100:
        # Small dataset - test wide range since lower values might produce all noise
         = [15.0, 20.0, 25.0, 30.0]
        min_samples_values = [2]
        perplexity_values = [10, 20, 30]
    elif num_emails < 300:
        # Medium dataset
         = [8.0, 12.0, 16.0, 20.0, 25.0]
        min_samples_values = [3, 4]
        perplexity_values = [15, 25, 35]
    else:
        # Large dataset
         = [10.0, 15.0, 20.0, 25.0, 30.0]
        min_samples_values = [4, 5]
        perplexity_values = [20, 30, 40]

    configs = []
    for eps in :
        for min_samples in min_samples_values:
            for perplexity in perplexity_values:
                configs.append(ParameterConfig(eps, min_samples, perplexity))

    return configs


def evaluate_cluster_coherence(
    clusters: list[ClusterMetadata],
    persona_id: int,
    guideline: Optional[str] = None
) -> tuple[float, str]:
    """
    Use GPT to evaluate how well cluster labels match the actual email content.

    Args:
        clusters: List of cluster metadata
        persona_id: Persona ID
        guideline: Optional natural language guideline for clustering (e.g., "Group by intent")

    Returns:
        (coherence_score, evaluation_details): Score 0-10 and GPT's reasoning
    """
    if not clusters:
        return 0.0, "No clusters formed - all emails classified as noise"

    # Filter out noise cluster
    real_clusters = [c for c in clusters if c.cluster_label != -1]

    if not real_clusters:
        return 0.0, "No real clusters - all emails classified as noise"

    # Get sample emails for each cluster
    cluster_summaries = []
    for cluster in real_clusters[:10]:  # Limit to 10 clusters to avoid token limits
        samples = db.get_cluster_samples(cluster.cluster_id)

        if samples:
            # Take first 3 samples
            sample_texts = []
            for sample in samples[:3]:
                sample_texts.append(f"Subject: {sample.subject}\nBody: {sample.body[:200]}...")

            cluster_summaries.append({
                "label": cluster.short_label or f"Cluster {cluster.cluster_label}",
                "description": cluster.description or "No description",
                "num_emails": cluster.num_emails,
                "samples": sample_texts
            })

    # Build GPT evaluation prompt
    if guideline:
        prompt = f"""You are evaluating the quality of email clustering results. Your job is to assess whether cluster labels accurately describe the emails within them.

CLUSTERING GOAL: {guideline}

The clusters should be organized according to this goal. Evaluate how well they achieve this objective.

For each cluster below, you'll see:
1. The cluster label (generated by AI)
2. The cluster description (generated by AI)
3. Sample emails from that cluster

Evaluate:
- Do the sample emails actually match the cluster label and description?
- Are the clusters meaningful and distinct from each other?
- Do the clusters align with the specified goal: "{guideline}"?
- Would these clusters be useful for understanding communication patterns?

"""
    else:
        prompt = """You are evaluating the quality of email clustering results. Your job is to assess whether cluster labels accurately describe the emails within them.

For each cluster below, you'll see:
1. The cluster label (generated by AI)
2. The cluster description (generated by AI)
3. Sample emails from that cluster

Evaluate:
- Do the sample emails actually match the cluster label and description?
- Are the clusters meaningful and distinct from each other?
- Would these clusters be useful for understanding communication patterns?

"""

    for i, cluster_summary in enumerate(cluster_summaries, 1):
        prompt += f"\n{'='*70}\n"
        prompt += f"CLUSTER {i}: {cluster_summary['label']}\n"
        prompt += f"Description: {cluster_summary['description']}\n"
        prompt += f"Size: {cluster_summary['num_emails']} emails\n\n"
        prompt += "Sample Emails:\n"
        for j, sample in enumerate(cluster_summary['samples'], 1):
            prompt += f"\nEmail {j}:\n{sample}\n"

    prompt += f"\n{'='*70}\n\nProvide your evaluation:\n\n"
    prompt += "1. Overall Coherence Score (0-10): How well do cluster labels match email content?\n"
    prompt += "   0 = Completely mismatched, 10 = Perfect categorization\n\n"
    prompt += "2. Brief Reasoning: Explain your score in 2-3 sentences.\n\n"
    prompt += "Format your response as:\n"
    prompt += "SCORE: [number]\n"
    prompt += "REASONING: [your explanation]"

    try:
        response_text, tokens_used = generate_text(
            prompt=[{"role": "user", "content": prompt}],
            model="gpt-4o-mini",
            temperature=0.3
        )

        # Parse response
        lines = response_text.strip().split('\n')
        score = 5.0  # Default
        reasoning = response_text

        for line in lines:
            if line.startswith("SCORE:"):
                try:
                    score = float(line.split("SCORE:")[1].strip())
                    score = max(0.0, min(10.0, score))  # Clamp to 0-10
                except ValueError:
                    pass
            elif line.startswith("REASONING:"):
                reasoning = line.split("REASONING:")[1].strip()

        return score, reasoning

    except Exception as e:
        logger.error(f"Failed to get GPT evaluation: {e}")
        return 5.0, f"Error during evaluation: {e}"


def calculate_distribution_score(clusters: list[ClusterMetadata], total_emails: int) -> float:
    """
    Calculate a score based on cluster size distribution.

    Good clustering:
    - Not too many tiny clusters
    - Not one giant cluster with everything
    - Balanced distribution
    - Low noise percentage

    Returns score 0-10
    """
    if not clusters:
        return 0.0

    real_clusters = [c for c in clusters if c.cluster_label != -1]
    noise_cluster = next((c for c in clusters if c.cluster_label == -1), None)

    if not real_clusters:
        return 0.0  # All noise

    noise_emails = noise_cluster.num_emails if noise_cluster else 0
    noise_pct = (noise_emails / total_emails * 100) if total_emails > 0 else 100

    # Penalize high noise
    noise_score = max(0, 10 - (noise_pct / 10))  # 0% noise = 10, 100% noise = 0

    # Check cluster count (3-15 is ideal)
    num_clusters = len(real_clusters)
    if num_clusters == 0:
        cluster_count_score = 0
    elif num_clusters < 3:
        cluster_count_score = num_clusters * 3.33  # Linear from 0 to 10
    elif num_clusters <= 15:
        cluster_count_score = 10
    else:
        cluster_count_score = max(0, 10 - (num_clusters - 15) * 0.5)

    # Check size distribution - avoid one giant cluster
    cluster_sizes = [c.num_emails for c in real_clusters]
    if cluster_sizes:
        largest_pct = (max(cluster_sizes) / total_emails * 100)
        if largest_pct > 70:  # One cluster has >70% of emails
            distribution_score = 0
        elif largest_pct > 50:
            distribution_score = 5
        else:
            distribution_score = 10
    else:
        distribution_score = 0

    # Weighted combination
    overall = (noise_score * 0.4 + cluster_count_score * 0.3 + distribution_score * 0.3)
    return overall


def optimize_parameters(
    persona_id: int,
    progress_callback=None,
    guideline: Optional[str] = None
) -> tuple[ParameterConfig, ClusterQuality]:
    """
    Auto-optimize clustering parameters for a persona.

    Tests multiple parameter configurations, evaluates each with GPT,
    and returns the best configuration.

    Args:
        persona_id: The persona to optimize for
        progress_callback: Optional callback(step, percent, config) for progress updates

    Returns:
        (best_config, best_quality): Best parameter configuration and its quality metrics
    """
    logger.info(f"Starting parameter optimization for persona {persona_id}")

    # Get email count to determine parameter grid
    with db.get_connection() as conn:
        cursor = conn.cursor()
        cursor.execute(
            "SELECT persona_name, total_emails FROM persona_indexes WHERE persona_id = ?",
            (persona_id,)
        )
        row = cursor.fetchone()
        if not row:
            # Need to check vdos.db for email count
            from virtualoffice.common.db import get_connection as get_vdos_connection
            with get_vdos_connection() as vdos_conn:
                vdos_cursor = vdos_conn.cursor()
                vdos_cursor.execute("""
                    SELECT p.name, p.email_address
                    FROM people p
                    WHERE p.id = ?
                """, (persona_id,))
                person_row = vdos_cursor.fetchone()
                if not person_row:
                    raise ValueError(f"Persona {persona_id} not found")

                persona_name, email_address = person_row
                vdos_cursor.execute(
                    "SELECT COUNT(*) FROM emails WHERE sender = ?",
                    (email_address,)
                )
                total_emails = vdos_cursor.fetchone()[0]
        else:
            persona_name, total_emails = row

    logger.info(f"Optimizing for {persona_name} with {total_emails} emails")

    # Generate parameter grid
    configs = generate_parameter_grid(total_emails)
    logger.info(f"Testing {len(configs)} parameter configurations")

    results = []
    all_results = []  # Track all results including all-noise configs for fallback

    for i, config in enumerate(configs):
        if progress_callback:
            progress_callback(
                f"Testing config {i+1}/{len(configs)}",
                (i / len(configs)) * 90,  # Reserve 10% for final selection
                config
            )

        logger.info(f"Testing configuration {i+1}/{len(configs)}: {config}")

        try:
            # Run clustering with this config
            engine = ClusterEngine(
                dbscan_eps=config.dbscan_eps,
                dbscan_min_samples=config.dbscan_min_samples,
                tsne_perplexity=config.tsne_perplexity
            )

            # Build index (this will save to database)
            engine.build_index(persona_id, lambda s, p: None)

            # Get results
            clusters = db.get_clusters_for_persona(persona_id)

            logger.info(f"Config {i+1}: Found {len(clusters)} total clusters")

            # Calculate metrics
            noise_cluster = next((c for c in clusters if c.cluster_label == -1), None)
            noise_emails = noise_cluster.num_emails if noise_cluster else 0
            noise_pct = (noise_emails / total_emails * 100) if total_emails > 0 else 100
            num_real_clusters = len([c for c in clusters if c.cluster_label != -1])

            logger.info(f"Config {i+1}: {num_real_clusters} real clusters, {noise_pct:.1f}% noise")

            # Only evaluate with GPT if we have actual clusters
            if num_real_clusters > 0:
                coherence_score, evaluation = evaluate_cluster_coherence(clusters, persona_id, guideline)
                distribution_score = calculate_distribution_score(clusters, total_emails)
                overall_score = (coherence_score * 0.6 + distribution_score * 0.4)
            else:
                # All noise - use noise percentage as negative score
                coherence_score = 0.0
                evaluation = "No clusters formed - all emails classified as noise"
                distribution_score = 0.0
                overall_score = -(noise_pct / 10.0)  # Negative score based on noise %

            quality = ClusterQuality(
                config=config,
                total_emails=total_emails,
                num_clusters=num_real_clusters,
                noise_percentage=noise_pct,
                coherence_score=coherence_score,
                distribution_score=distribution_score,
                overall_score=overall_score,
                evaluation_details=evaluation
            )

            all_results.append(quality)

            # Only add to results if it produced actual clusters
            if num_real_clusters > 0:
                results.append(quality)
                logger.info(f"Config {i+1} score: {overall_score:.2f}/10 âœ“")
            else:
                logger.warning(f"Config {i+1} produced no clusters (all noise)")

        except Exception as e:
            logger.error(f"Failed to test config {config}: {e}")
            continue

    # Find best configuration
    if results:
        # Use best configuration that produced actual clusters
        best_quality = max(results, key=lambda q: q.overall_score)
        logger.info("Selected best configuration from cluster-producing configs")
    elif all_results:
        # Fallback: all configs produced only noise, pick the one with least noise
        best_quality = min(all_results, key=lambda q: q.noise_percentage)
        logger.warning(
            f"All configurations produced only noise! "
            f"Using config with lowest noise: {best_quality.noise_percentage:.1f}%"
        )
    else:
        raise ValueError("No configurations produced valid results")

    if progress_callback:
        progress_callback("Selecting best configuration", 95, best_quality.config)

    logger.info(f"Best configuration: {best_quality.config} with score {best_quality.overall_score:.2f}/10")

    # Re-run with best config to ensure it's saved in database
    engine = ClusterEngine(
        dbscan_eps=best_quality.config.dbscan_eps,
        dbscan_min_samples=best_quality.config.dbscan_min_samples,
        tsne_perplexity=best_quality.config.tsne_perplexity
    )
    engine.build_index(persona_id, lambda s, p: None)

    if progress_callback:
        progress_callback("Optimization complete", 100, best_quality.config)

    return best_quality.config, best_quality
